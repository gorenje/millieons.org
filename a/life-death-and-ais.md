---
permalink: /a/life-death-and-ais
---

## Are we prepared to live in world where algorithms decide on life and death?

Basis of this is an episode of [philosophie](https://www.arte.tv/de/videos/RC-014080/philosophie/) - [How to Solve a Moral Dilemma](https://www.arte.tv/en/videos/098794-001-A/how-to-solve-a-moral-dilemma/). The episode discusses the [trolley problem](https://en.wikipedia.org/wiki/Trolley_problem) in the context of self driving cars (approximately twelve-and-a-half mins in).

Quoting from Wikipedia, the trolley problem is:

> The trolley problem is a series of thought experiments in ethics and psychology, involving stylized ethical dilemmas of whether to sacrifice one person to save a larger number.

In the context of self driving cars, this poses uncomfortable questions on what happens in case of an accident? What happens if there is a decision to be made who has to die? I.e. either the 3 people on the side of the street or the one on the street in the middle of the road. Or the people sitting in the car?

And who is responsible? In some jurisdictions the answer has been: there has to be human driver that can override in such situation. The driver is then responsible. But why have self driving cars then? As the driver has to be permanently ensuring that no accident can happen.

What happens in jurisdictions where complete self driving cars are allowed? The Artificial Intelligence (AI) controlling the car makes the final decision what happens. The AI is making the decision over life and death of humans.

The unfortunate truth about AIs is that they are nothing other than extremely complex computer programs that have been trained on some input data to perform some task. They are that complex that we don't understand how exactly how they make their decisions and we are unable to diagnosis and understand their decision making after the fact.

### No more car accidents?

We will see a growing reorganisation of the urban landscape towards making it easier for self-driving cars to navigate our streets. As, interestingly enough, jaywalking was made an offence by [lobbying of the early car industry](https://web.archive.org/web/20210212205534/https://www.bbc.com/news/magazine-26073797).

However it is doubtful that there will no more car accidents or deaths related to self driving cars. Just imagine children suddenly running onto streets. Unless of course cars, similar to underground trains, are placed underground. However it is highly impractical to build tunnels everywhere were people wish to go.

The questions remain: *are we fine with AI making decisions on the life and death of humans **and** are we fine with not being able to reconstruct why the AI made the decision?*

### How to deal with the 'why'?

To consider this question, we also have to especially who is responsible for an accident. The obvious answer is the manufacturer. But it's not that simple. For why this is the case, we only have to consider what happens with a plane crashes or fails.

### Planes, Trains and Automobiles

When planes crash killing hundreds of people, the insurance companies have to pay compensation to families and loved ones of the dead. To decide which insurance companies, the fault of the crash needs to be established. So there is a definite interest to get to the bottom of what caused a plane to crash.

Expensive recovery operations to retrieve planes from the bottom of oceans (not to mention finding them first) are launched. Then potentially whole fleets of planes are grounded because manufacturers are held liable if it is shown that a manufacturing issue caused failure.

If it turns out it was pilot error, the airline pays the compensation. If the anti-icing routines of planes at airports were the cause, then the airport pays.

The same thing will happen with self-driving cars. Eventually there will be blackboxes in self-driving cars and entire fleets will be grounded until either hardware or software upgrades can be made.

However how can an AI be debugged? Normally if an AI is making wrong decisions, it's training data is checked for anomalies and the entire AI is retrained. This an expensive task. In addition, it doesn't mean the AI won't make other errors.

### Self reflection?

By self reflection I mean a meta-communication system with an AI. Something like human speech, for human beings. Will we be motivated to build a communication system for AIs so that we can explain and discuss complex scenarios with an AI? A kind of diagnostic communication protocol where the AI could self reflect on itself to answer the question "why".

Having a self reflection mechanism is something that would obviously make diagnosis simpler. However how can we then modify the AI to correct the behaviour? This leads to the need of a self-healing system for an AI.

Combining a self-reflecting and self-healing mechanism would mean that an AI could make changes to itself - without our intervention. This is probably the point of [singularity](https://en.wikipedia.org/wiki/Technological_singularity) and as described by [Nick Bostrom](https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies), this is very hard to control.

I guess the last question I would pose is whether self-driving cars are on the road that leads us to singularity?
